# -*- coding: utf-8 -*-
"""Meta_model_Graphs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n6r2NIocFYOvKFN0K0plbLRCUBmNSzy9

# **Install Required Packages**
"""

!pip install torch_geometric

!pip install gensim==4.1.2 scipy==1.7.3 torch_geometric

# Step 1: Import libraries
import os
import glob
import yaml
import csv
import numpy as np
import pandas as pd
import torch
import networkx as nx
from pathlib import Path
from sklearn.preprocessing import OneHotEncoder
from torch_geometric.data import Data, HeteroData
from networkx.readwrite import json_graph
# Uncomment the following lines if these modules are part of your project
# from extractElements import extractElements
# from create_graph import CreateGraph
# from load_yml import ModelConfigLoader
# from GCN_GAT import GCN, GAT
# from gensim.models import Word2Vec

"""# **Check if GPU is available**"""

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

"""graph to data & data to graph"""

edge_types = []



class Node:
  def __init__(self, filename):
    self.data, self.attributes = self.load_data(filename)

  def load_data(self, filename):
    df = pd.read_csv(filename)
    filename = Path(filename).name
    node_type = filename.rsplit("_", 1)[0]
    src = node_type.split("_")[0]
    id_col = f"{src}Id"
    attributes = df[list(df.columns)[1:]]
    data = df[[id_col, 'graphNumber']]
    return data, attributes

class Edge:
  def __init__(self, filename):
    self.data, self.attributes = self.load_data(filename)

  def load_data(self, filename):
      df = pd.read_csv(filename)
      filename = Path(filename).name
      edge_type = filename.rsplit("_", 1)[0]

      # edge_type = Path(filename).rsplit("_", 1)[0]

      m = re.match(r"^(.*)_(.*)$", edge_type)
      if m:
          parts = m.groups()
      else:
          raise ValueError(...)

      src = parts[0]
      dst = parts[1]

      src_col = f"sr{src}Id"
      dst_col = f"ds{dst}Id"
      attributes = list(df.columns)[2:]
      data = df[[src_col, dst_col]]
      return data, attributes
class CreateGraph:
    def __init__(self, data_path, filters):
        self.nodes = {}
        self.edges = {}
        self.filters = filters
        self.metamodel_config = []


        node_files = self.get_node_files(data_path)
        for file in node_files:
            filename = Path(file).name
            node_type = filename.rsplit("_", 1)[0]

            # edge_type = file?name.rsplit("_", 1)[0]
            self.nodes[node_type] = Node(file)

        edge_files = self.get_edge_files(data_path)
        for file in edge_files:
            filename = Path(file).name
            edge_type = filename.rsplit("_", 1)[0]
            self.edges[edge_type] = Edge(file)
        self.edit_data()


    #edit nodes and edges data according ti Yaml file
    def edit_data(self):
        nodes_copy = self.nodes.copy()
        edges_copy = self.edges.copy()


        for node_type, node in nodes_copy.items():
            include = self.filters.get("classes").get("include")

            exclude = self.filters.get("classes").get("exclude")
            if (include is not None and node_type not in include) or (exclude is not None and node in exclude):
                del self.nodes[node_type]

            # include or exclude the attributes of classes
            # By Default, the attributes of nodes are not considered
            # But they are considered if includeAllAttributes = true

            if self.filters.get("classes").get("includeAllAttributes") == False or self.filters.get("classes").get("excludeAllAttributes") == True:
                if self.nodes.get(node_type) is not None:
                    self.nodes[node_type].attributes = []

        for edge in edges_copy:
            # print("edges", edge)
            edge_type_parts = edge.split("_")
            source_node=edge_type_parts[0]
            destination_node = edge_type_parts[1]
            if include is not None and (source_node and destination_node) not in include:
                del self.edges[edge]
            if exclude is not None and (source_node or destination_node) in exclude:
                del self.edges[edge]


    def get_node_files(self, data_path):
        return glob.glob(f"{data_path}/nodes/*.csv")

    def get_edge_files(self, data_path):
        return glob.glob(f"{data_path}/edges/*.csv")

    def get_edge_type(self, filename):
        return Path(filename).stem

    def onehot_encode(self, encoding, col, value):
        unique = list(encoding[col]["unique_values"])
        num_rows = len(unique)

        # Reshape based on number of rows
        unique = np.reshape(unique, (num_rows, 1))

        encoder = OneHotEncoder()
        encoder.fit(unique)
        val = np.array(value)

        val = val.reshape(1, -1)
        new_val = encoder.transform(val).toarray()
        return new_val

    def word2vec_encode(self, encoding, col, value):
        unique_names = encoding[col]["unique_values"]
        names = [[word] for word in unique_names]
        modelnamestates = Word2Vec(names, min_count=1)
        modelnamestates.save('word2vec.modelnamestate')
        import gensim
        modelnamestates = gensim.models.Word2Vec.load('word2vec.modelnamestate')
        all_embeddings_namestates = {}
        for word in unique_names:
            if word in modelnamestates.wv:
                embedding = modelnamestates.wv[word]
                all_embeddings_namestates[word] = embedding

        new_val = all_embeddings_namestates[value]
        return new_val

    def worde4mde (self, value):
        sgram_mde = load_embeddings('sgram-mde')
        new_val = sgram_mde[value]
        return new_val

    def model2heterograph(self) -> HeteroData:
        graphs = {}
        # data = HeteroData()

        for node_type, node in self.nodes.items():
            encoding = {}
            # unique_values = set()
            columns = list(node.attributes)
            for column in columns:
                if self.filters.get('classes', {}).get(node_type, {}).get('features', {}).get(column, {}).get(
                        'encoding') is not None:
                    encoding[column] = {}
                    encoding[column]["enc_type"] = self.filters.get('classes', {}).get(node_type, {}).get('features',
                                                                                                          {}).get(
                        column, {}).get(
                        'encoding')
                    unique_values = set(node.attributes[column])
                    encoding[column]["unique_values"] = unique_values
            for _, row in node.data.iterrows():
                graphNum = row["graphNumber"]
                if graphs.get(graphNum) is None:
                    G = HeteroData()
                    graphs[graphNum] = G
                else:
                    G = graphs[graphNum]

                id = node_type.split("_")[0]
                attr_dict = {}

                for col in node.attributes.columns:
                    for val in node.attributes[col]:
                        if encoding.get(col) is not None:
                            if encoding[col]["enc_type"] == "one-hot":
                                new_val = self.onehot_encode(encoding,col, val)
                            if encoding[col]["enc_type"] == "word2vec":
                                new_val= self.word2vec_encode(encoding, col, val)
                            if encoding[col]["enc_type"] == "worde4mde":
                                new_val = self.worde4mde(val)
                            val = new_val
                        attr_dict[col] = val
                G[node_type].x = attr_dict

        for edge_type, edge in self.edges.items():
            for _, row in edge.data.iterrows():
                src = edge_type.split("_")[0]
                dst = edge_type.split("_")[1]
                G[edge_type].edge_index = self.get_edge_index(edge.data, src, dst)
                G[edge_type].attrs = edge.attributes

        for edge_type, edge in self.edges.items():
            src = edge_type.split("_")[0]
            dst = edge_type.split("_")[1]

            G[edge_type].edge_index = self.get_edge_index(edge.data, src, dst)
            # data[edge_type].edge_index = self.get_edge_index(edge.data)
            G[edge_type].attrs = edge.attributes


        return graphs

    def get_edge_index(self, edge_data, src, dst):
        src_col = f"sr{src}Id"
        dst_col = f"ds{dst}Id"
        return edge_data[[src_col, dst_col]].values

    def model2HomoGraph(self) -> nx.Graph:
        graphs = {}
        ids = {}


        # Add nodes
        for node_type, node in self.nodes.items():
            encoding = {}
            columns = list(node.attributes)
            for column in columns:
                if self.filters.get('classes', {}).get(node_type, {}).get('features', {}).get(column, {}).get(
                    'encoding') is not None:
                    encoding[column] = {}
                    encoding[column]["enc_type"] = self.filters.get('classes', {}).get(node_type, {}).get('features', {}).get(column, {}).get(
                    'encoding')
                    unique_values = set(node.attributes[column])
                    encoding[column]["unique_values"] = unique_values

            index = 0
            for _, row in node.data.iterrows():
                graphNum = row["graphNumber"]
                # Create graph if not exists
                if graphs.get(graphNum) is None:
                    G = nx.MultiDiGraph()
                    graphs[graphNum] = G
                    ids[graphNum] =0

                    # Create new graph
                    # Add to graphs list
                else:
                    # Get existing graph

                    G = graphs[graphNum]
                id = node_type.split("_")[0]
                nid = row[f"{id}Id"]
                node.data.loc[index, 'newId'] = ids[graphNum]
                index += 1
                newId = ids[graphNum]
                ids[graphNum] += 1

                G.add_node(newId)
                G.nodes[newId]['type'] = node_type
                for col in node.attributes.columns:
                    for val in node.attributes[col]:
                        if encoding.get(col) is not None:
                            if encoding[col]["enc_type"] == "one-hot":
                                new_val = self.onehot_encode(encoding,col, val)
                            if encoding[col]["enc_type"] == "word2vec":
                                new_val= self.word2vec_encode(encoding, col, val)
                            if encoding[col]["enc_type"] == "worde4mde":
                                new_val = self.worde4mde(val)

                                # new_val = new_val.reshape(1, -1)
                            val = new_val

                        G.nodes[newId][col] = val


        # Add edges
        for edge_type, edge in self.edges.items():

            for _, row in edge.data.iterrows():
                part1 = edge_type.split("_")[0]
                part2 = edge_type.split("_")[1]
                src = row[f"sr{part1}Id"]
                dst = row[f"ds{part2}Id"]

                newsrc = int(self.nodes[part1].data.loc[self.nodes[part1].data[f"{part1}Id"] == src, 'newId'].iloc[0])
                newdst = int(self.nodes[part2].data.loc[self.nodes[part2].data[f"{part2}Id"] == dst, 'newId'].iloc[0])

                graph_number =  self.nodes[part1].data.loc[self.nodes[part1].data[f"{part1}Id"] == src, 'graphNumber'].iloc[0]
                print("before:")
                print("g nodes:", G.nodes)
                graphs[graph_number].add_edge(newsrc, newdst)


        return graphs

"""# **Extract elements**"""

import csv
import glob
import os
from collections.abc import Iterable
# from org.eclipse.emf import ecore
# from pyecore.ecore import EAttribute
# from pyecore.ecore import EStructuralFeature
# from pyecore.ecore import EOrderedSet
# from pyecore.ecore import EClass

# from ecore import EAttribute

# import networkx as nx
import yaml
# from pyecore.ecore import EEnum
# from pyecore.resources import ResourceSet, URI

import csv
import os
class extractElements:
    case = "xmi"
    ecoreNumber = 0
    featureTypes = []
    graphNo = 0
    nodeLabel = 1
    Graphs = []
    attributeNames = {}
    classes = dict()
    directories = 1
    filenumber = 0
    yamlexistance = 0
    mapvalue = {}
    yamlcontents = {}
    referenceNames = {}
    IDs = {}
    graphAttributes = {}
    visitednodes = []
    jsondata = {}
    visited = []
    queue = []
    pathfile = None
    yamlFeatures = {}
    yamlclasses = {'include': [], 'exclude': []}
    modelData = {}
    csvData = {}
    refFeatures = []
    csvEdgesData = {}
    folder = ""
    ids = {}

    graphNumber = 0

    def __init__(self, models_path: str, metamodel_path:str):
        self.models_path = models_path
        self.metamodel_path = metamodel_path

        self.load_config()
        self.load_metamodel(metamodel_path)
        self.process_models(models_path)

        #check the format of input models in Yaml
    def load_config(self) -> None:

        self.current_dir = os.path.dirname(__file__)
        config_file = os.path.join(self.current_dir, 'config.yaml')
        with open(config_file) as f:
            self.yamlcontents = yaml.load(f, Loader=yaml.FullLoader)
        self.case = self.yamlcontents['inputmodels']['format']

    def load_metamodel(self, metamodel_path) -> None:
        rset = ResourceSet()
        resource = rset.get_resource(URI(metamodel_path))
        self.meta_root = resource.contents[0]

    def process_models(self, models_path) -> None:
        if self.case == "ecore":
            self.load_ecore_models(models_path)
        elif self.case == "xmi":
            self.load_xmi_models(models_path)
        self.create_edges()
        self.create_csv()

    def load_ecore_models(self, models_path):

        os.chdir(models_path)

        for file in glob.glob("*.ecore"):
            print("file is", file)
            ecore_file = os.path.join(models_path, file)

            self.rset = ResourceSet()
            resource = self.rset.get_resource(URI(ecore_file))
            model_root = resource.contents[0]

            self.ecoreNumber += 1
            self.create_nodes(model_root, self.meta_root)

    def load_xmi_models(self, models_path):

        os.chdir(models_path)

        self.rset.metamodel_registry[self.meta_root.nsURI] = self.meta_root

        for file in glob.glob("*.xmi"):
            xmi_file = os.path.join(models_path, file)

            resource = self.rset.get_resource(URI(xmi_file))
            model_root = resource.contents[0]

            self.filenumber += 1
            self.create_nodes(model_root, self.meta_root)



    def checkYaml(self, mmroot, model, elementType, case, element=None, ):
        print("checkyaml is", "mmroot=", mmroot, "model is=", model, "elementType=", elementType, "case=", case,
              "element=", element)
        metamodel_name = mmroot.name
        metamodel_nsURI = mmroot.nsURI
        modelName = model.eClass.name
        print("element is=", element)
        if element != None:
            elementName = element.name
        # print("model in checkyaml:", modelName)
        if self.yamlexistance == 0:
            print("yaml 0")
            return True
        elif case == "include" and elementType == "class":
            print("checking existance")

            print("meta name=", metamodel_name)
            print("ns is", metamodel_nsURI)
            print("modelName=", modelName)
            if ((self.deep_get(self.yamlcontents,
                               ['adaptations', 'metamodels', 'packages', metamodel_name,
                                'uri', mmroot.nsURI, 'classes',
                                'exclude']) != None and modelName in self.deep_get(
                self.yamlcontents,
                ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI, 'classes',
                 'exclude'])) or (self.deep_get(self.yamlcontents,
                                                ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri',
                                                 mmroot.nsURI, 'classes',
                                                 'include']) != None and modelName not in self.deep_get(
                self.yamlcontents,
                ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI, 'classes',
                 'include']))):
                print("model True")
                return False
            else:
                return True
        elif elementType == "feature" and case == "existance":
            print("here feature existance")
            if self.deep_get(self.yamlcontents,
                             ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                              'classes', modelName, 'features', elementName, 'isIncluded']) != None:
                if self.deep_get(self.yamlcontents,
                                 ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                                  'classes', modelName, 'features', elementName, 'isIncluded']) == "False":
                    print("return False1")
                    return False





            elif (self.deep_get(self.yamlcontents,
                                ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                                 'classes', modelName, "include"]) != None and elementName not in self.deep_get(
                self.yamlcontents, ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                                    'classes', modelName, "include"])):
                print("return False2")
                return False
            elif self.deep_get(self.yamlcontents,
                               ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                                'classes', "excludeAllAttributes"]) == "True":
                print("return False3")
                return False
            else:
                print("return True")
                return True
        elif case == "renaming":
            print("yamlcontent::", self.yamlcontents)
            print("renaming")
            print("metaname:", metamodel_name)
            print("model", modelName)
            print("element:", elementName)
            print("rename value==", self.deep_get(self.yamlcontents,
                                                  ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri',
                                                   mmroot.nsURI,
                                                   'classes', modelName, 'features', elementName, 'renaming']))
            if self.deep_get(self.yamlcontents,
                             ['adaptations', 'metamodels', 'packages', metamodel_name, 'uri', mmroot.nsURI,
                              'classes', modelName, 'features', elementName, 'renaming']) != None:
                print("rename exists")
                return self.yamlcontents['adaptations']['metamodels']['packages'][metamodel_name]['uri'][mmroot.nsURI][
                    'classes'][modelName]['features'][elementName]['renaming']
            else:

                return True

    def deep_get(self, d, keys):
        if not keys or d is None:
            return d
        return self.deep_get(d.get(keys[0]), keys[1:])

    model_data = {}
    attribute_types = []

    def create_nodes(self, model, mmroot):
        print("creating nodes")
        self.graphNumber = self.graphNumber + 1
        print("graphNumber issss:", self.graphNumber)
        # Data structures
        self.model_data = {}
        self.attribute_types = []

        # Create node data for root model

        node_data = self.create_node_data(model, mmroot, self.graphNumber)


        # Add root node data to model data
        self.add_to_model_data(model, node_data)

        # Process child elements
        for element in self.get_child_elements(model):
            print("child element")
            # Create node data for child

            child_node_data = self.create_node_data(element, mmroot, self.graphNumber)

            # Add child node data to model data
            self.add_to_model_data(element, child_node_data)

        # Return data structures
        return self.model_data, self.attribute_types

    def create_node_data(self, element, mmroot, graphNo):
        print("creating node data")

        graphdata = {}
        graphcsvdata = {}

        # metamodel_name = mmroot.name
        # modelName = element.eClass.name

        node_number = 0
        if element.eClass.name in self.ids:
            node_number = self.ids[element.eClass.name] + 1

        self.ids[element.eClass.name] = node_number

        for feature in element._isset:


            feature_name = feature.name
            feature_value = element.eGet(feature)

            graphdata[feature_name] = feature_value
            # print("graphNo is now:", graphNo)
            graphdata["graphNumber"] = graphNo
            graphcsvdata["graphNumber"] = graphNo

            if not feature.is_reference:
                graphcsvdata[feature_name] = feature_value

            if feature.is_attribute:

                types = {}
                types["node"] = element.eClass.name
                types["name"] = feature.name
                print("ids is:", self.ids)
                print("element is", element)
                types["nodeNumber"] = self.ids[element.eClass.name]
                # node_number = self.ids.get(class_name, 0) + 1

                if isinstance(feature.eType, EEnum):
                    types["type"] = "EString"
                else:
                    types["type"] = feature.eType.name

                self.featureTypes.append(types)
                types = {}

        self.modelData.setdefault(element.eClass.name, []).append(graphdata)
        self.modelData[element.eClass.name][-1]["element"] = element

        self.csvData.setdefault(element.eClass.name, []).append(graphcsvdata)


        return graphdata



    def get_node_number(self, element):
        print("csvdata", self.csvData)
        print(" modeldata", self.modelData)
        class_name = element.eClass.name

        node_number = self.ids.get(class_name, 0) + 1

        self.ids[class_name] = node_number

        return node_number

    def add_to_model_data(self, element, data):
        print("add to model data")


        self.model_data.setdefault(element.eClass.name, []).append(data)
        data["element"] = element

    def get_child_elements(self, element):

        return element.eAllContents()

    def get_eclass(self, element, mmroot):

        return mmroot.getEClassifier(element.eClass.name)

    def get_features(self, eclass):

        return list(eclass.eStructuralFeatures)



    def create_edges(self):
        print("Creating edges...")
        for keys in self.csvData:
            for i in range(0, len(self.csvData[keys])):
                        id = str(keys) + "Id"
                        self.csvData[keys][i][id] = i
                        self.modelData[keys][i][id] = i

        reference_properties = {"eClassifiers", "ePackage", "eStructuralFeatures", "eType",
                        "eContainingClass", "eOpposite", "eSuperTypes", "eModelElement"}

        for node_type, nodes in self.modelData.items():
            # print("nodes is", nodes)
            for node in nodes:

                for property, references in node.items():
                    if property in reference_properties:

                        # print("references", references)
                        if isinstance(references, Iterable):
                            # print("yes iterable")
                            refs = list(references)
                        else:
                            refs = [references]

                        source_id = node[f"{node_type}Id"]
                        graph_number = node["graphNumber"]
                        # print("references", refs)
                        for reference in refs:

                            edge_type = reference.eClass.name

                            if self.csvEdgesData.get(f"{node_type}_{edge_type}") is None:
                                self.csvEdgesData[f"{node_type}_{edge_type}"] = []
                            # print("edge type", edge_type)
                            # print("model data:", self.modelData)
                            if edge_type in self.modelData:

                                for dest_node in self.modelData[edge_type]:

                                    if ("element", reference) in dest_node.items() and (
                                    dest_node["graphNumber"], graph_number):
                                        dest_id = dest_node[f"{edge_type}Id"]

                                        edge_data = {}
                                        edge_data[f"sr{node_type}Id"] = source_id
                                        edge_data[f"ds{edge_type}Id"] = dest_id
                                        edge_data["name"] = property

                                        self.csvEdgesData[f"{node_type}_{edge_type}"].append(edge_data)



    def create_csv(self):



        print("Creating CSV files...")

        # Encapsulate node logic
        self._write_nodes_csv()

        # Encapsulate edges logic
        self._write_edges_csv()

        # Encapsulate types logic
        self._write_types_csv()

        print("CSV files created!")



    def _write_nodes_csv(self):
        for types, records in self.csvData.items():
            self._write_to_csv(records, types, "nodes")

    def _write_edges_csv(self):
        for types, records in self.csvEdgesData.items():
            if records:
                self._write_to_csv(records, types, "edges")

    def _write_types_csv(self):
        print("feature types:", self.featureTypes)
        self._write_to_csv(self.featureTypes, "types", folder="types")

    def _write_to_csv(self, records, types, folder):
        if types == "types":
            filename = f"{types}"
        else:
            filename = f"{types}_{folder}"
        dir_path = f"..\\csvfiles\\{folder}"

        if not os.path.isdir(dir_path):
            os.makedirs(dir_path)

        fields = records[0].keys()

        with open(os.path.join(dir_path, f'{filename}.csv'), 'w', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=fields, extrasaction='ignore')
            writer.writeheader()

            writer.writerows(records)

"""# visualize graph"""

import matplotlib.pyplot as plt
import matplotlib as mpl
import networkx as nx
import graphviz

class GraphVisualizer:
    def __init__(self, G):
        self.G = G

    def get_map_attention(self, atts):
        return {n: atts[i].detach().cpu().numpy() for i, n in enumerate(self.G.nodes())}

    def get_colors(self, map_colors):
        low, *_, high = sorted(map_colors.values())
        norm = mpl.colors.Normalize(vmin=low, vmax=high, clip=True)
        mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)
        return {n: '#%02x%02x%02x' % tuple(mapper.to_rgba(c, bytes=True)[:3]) for n, c in map_colors.items()}

    def heat_map(self, atts, filename, directory, ignore=[]):
        g = graphviz.Digraph(filename)
        colors = self.get_colors(self.get_map_attention(atts))
        for e in self.G.edges(data=True):
            if any(kw in e[2]['type'] for kw in ignore):
                continue
            for node in e[:2]:
                g.node(str(node), label=self.G.nodes[node]['type'],
                       color=colors[node], style='filled', fillcolor=colors[node])
            g.edge(str(e[0]), str(e[1]), label=e[2]['type'])
        g.format, g.directory = 'pdf', directory
        g.view()

# Create a sample graph
G = nx.Graph()
G.add_nodes_from('/content/MetaModel_EXTRA/csvfiles/nodes') # change location
G.add_edges_from([(0, 1, {'type': 'edge_type_1'}), (1, 2, {'type': 'edge_type_2'})])#change location

# Create an instance of GraphVisualizer
visualizer = GraphVisualizer(G)

# # Example usage of methods

"""# early stopping"""

import torch

class EarlyStopping:
    def __init__(self, patience=5, min_delta=0, mode='min', path='checkpoint.pth'):
        self.patience, self.min_delta, self.mode = patience, min_delta, mode
        self.best, self.num_bad_epochs = None, 0
        self.is_better = (lambda a, b: a < b - min_delta) if mode == 'min' else (lambda a, b: a > b + min_delta)
        self.path = path

    def step(self, metrics, model, epoch):
        if self.best is None or self.is_better(metrics, self.best):
            self.best, self.num_bad_epochs = metrics, 0
            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'metrics': metrics}, self.path)
        else:
            self.num_bad_epochs += 1
        return self.num_bad_epochs >= self.patience

import torch.optim as optim
import torch.nn as nn

# Example model and optimizer
model = nn.Linear(10, 2)  # Replace with your model
optimizer = optim.Adam(model.parameters())

# Initialize EarlyStopping with the corrected parameters
early_stopping = EarlyStopping(patience=5, min_delta=0.01, mode='min', path='checkpoint.pth')

# Simulated training loop
for epoch in range(100):
    # Assume you have some validation metric to monitor, such as validation loss
    val_loss = 0.1  # Replace with actual validation loss value

    # Check for early stopping
    if early_stopping.step(val_loss, model, epoch):  # Pass model and epoch within the loop
        print("Early stopping triggered")
        break

"""# **Enhancements and Next Steps**"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Extract the ZIP File**"""

import os
import zipfile

# Path to your zip file on Google Drive
zip_file_path = '/content/drive/MyDrive/gnnmodel/metamodel.zip'
extract_to = '/content/MetaModel_EXTRA'

# Ensure the extraction directory exists
os.makedirs(extract_to, exist_ok=True)

# Extract the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print("File extracted successfully!")

"""# **Load and Parse the YAML Configuration**"""

# load yml

import yaml

class ModelConfigLoader:
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = None
        self.model_type = None
        self.hidden_layers = None
        self.layer_dims = None
        self.activation_function = None
        self.learning_rate = None
        self.optimizer_type = None
        self.epochs = None
        self.batch_size = None
        self.use_node_features = None
        self.use_edge_features = None
        self.dropout = None
        self.train_test_split = None
        self.output_model_path = None

    def load_config(self):
        # Load the YAML configuration file
        with open(self.config_path, 'r') as file:
            self.config = yaml.safe_load(file)

        # Extract parameters from the loaded configuration
        self.model_type = self.config['model_type']
        self.hidden_layers = self.config['hidden_layers']
        self.layer_dims = self.config['layer_dimensions']
        self.activation_function = self.config['activation_function']

        self.learning_rate = self.config['learning_rate']
        self.optimizer_type = self.config['optimizer']
        self.epochs = self.config['epochs']
        self.batch_size = self.config['batch_size']
        self.use_node_features = self.config['use_node_features']
        self.use_edge_features = self.config['use_edge_features']
        self.dropout = self.config['dropout']
        self.train_test_split = self.config['train_test_split']
        self.output_model_path = self.config.get('output_model_path', './saved_model.pth')  # Default path

    def get_config(self):
        if self.config is None:
            raise ValueError("Configuration has not been loaded. Call 'load_config()' first.")
        return {
            'model_type': self.model_type,
            'hidden_layers': self.hidden_layers,
            'layer_dims': self.layer_dims,
            'activation_function': self.activation_function,
            'learning_rate': self.learning_rate,
            'optimizer_type': self.optimizer_type,
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'use_node_features': self.use_node_features,
            'use_edge_features': self.use_edge_features,
            'dropout': self.dropout,
            'train_test_split': self.train_test_split,
            'output_model_path': self.output_model_path
        }

"""# **Define the GCN and GAT Models**"""

!pip install torch_geometric

import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool

class GNNWithAttention(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GNNWithAttention, self).__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=4)
        self.conv2 = GATConv(hidden_channels * 4, hidden_channels, heads=4)
        self.pool = global_mean_pool
        self.fc = torch.nn.Linear(hidden_channels * 4, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.pool(x, data.batch)
        return F.log_softmax(self.fc(x), dim=1)

from torch_geometric.nn import GATConv
# Define the GAT model
class GAT(nn.Module):
    def __init__(self, num_layers, layer_dims, activation_func):
        super(GAT, self).__init__()
        self.layers = nn.ModuleList()
        self.activation = getattr(nn, activation_func)()

        for i in range(num_layers):
            in_dim = layer_dims[i] if i == 0 else layer_dims[i - 1]
            out_dim = layer_dims[i]
            self.layers.append(GATConv(in_dim, out_dim))

    def forward(self, x, edge_index):
        for layer in self.layers:

            x = layer(x, edge_index)
            x = self.activation(x)
        return x

"""# **Graph Generation (New Component)**"""

# Graph Decoder to generate new graphs from latent embeddings
class GraphDecoder(torch.nn.Module):
    def __init__(self, latent_dim, num_node_features, num_nodes):
        super(GraphDecoder, self).__init__()
        self.latent_dim = latent_dim
        self.num_node_features = num_node_features
        self.num_nodes = num_nodes

        # A decoder that maps the latent embedding back to node features
        self.fc = nn.Linear(latent_dim, num_nodes * num_node_features)  # Create node features from latent embedding

    def forward(self, latent_embedding):
        # Decode latent embedding into node features
        decoded_node_features = self.fc(latent_embedding)
        decoded_node_features = decoded_node_features.view(self.num_nodes, self.num_node_features)

        # Generate a random edge index (this part can be modified for specific use-cases)
        edge_index = torch.randint(0, self.num_nodes, (2, self.num_nodes * 2))  # Example edge generation

        return decoded_node_features, edge_index

"""# **Training the Model with Graph Generation**"""

# Define the complete GNN Autoencoder model
class GNN_Autoencoder(torch.nn.Module):
    def __init__(self, gcn_encoder, graph_decoder):
        super(GNN_Autoencoder, self).__init__()
        self.encoder = gcn_encoder
        self.decoder = graph_decoder

    def forward(self, x, edge_index):
        # Encode the graph into a latent representation
        node_embeddings, latent_embedding = self.encoder(x, edge_index)

        # Decode the latent representation to generate a new graph
        decoded_node_features, decoded_edge_index = self.decoder(latent_embedding)

        return decoded_node_features, decoded_edge_index

"""# **Graph-to-Model Conversion (Reverse Process)**"""

class CreateGraph:
    # Add a reverse function that maps generated graphs back into model format
    def graph_to_model(self, graphs):
        models = []
        for graph_num, G in graphs.items():
            model = {}  # Dictionary to hold the model structure

            for node in G.nodes(data=True):
                node_id, node_attrs = node
                node_type = node_attrs['type']

                # Map graph nodes to model classes
                if model.get(node_type) is None:
                    model[node_type] = []
                model[node_type].append(node_attrs)

            for edge in G.edges(data=True):
                src, dst, edge_attrs = edge
                src_type = G.nodes[src]['type']
                dst_type = G.nodes[dst]['type']

                # Map graph edges to model relationships
                relationship = {'src': src, 'dst': dst, 'type': edge_attrs['type']}
                model[f'{src_type}_{dst_type}'].append(relationship)

            models.append(model)

        return models  # Return a list of models created from the graphs

"""# **Optimization and Loss Function**"""

# Loss function for reconstruction
def reconstruction_loss(original_node_features, generated_node_features, original_edge_index, generated_edge_index):
    # Loss for node feature reconstruction (MSE Loss)
    node_loss = torch.nn.functional.mse_loss(original_node_features, generated_node_features)

    # Loss for edge reconstruction (Cross Entropy or Binary Cross Entropy for edge prediction)
    edge_loss = torch.nn.functional.binary_cross_entropy_with_logits(
        torch.flatten(generated_edge_index.float()), torch.flatten(original_edge_index.float())
    )

    return node_loss + edge_loss  # Combine both losses

"""# **Training Pipeline**"""

def train_c2st(model, train_loader, val_loader, optimizer, epochs, patience=5):
    early_stopping = EarlyStopping(patience=patience, mode='max')
    for epoch in range(epochs):
        model.train()
        for data in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, data.y)
            loss.backward()
            optimizer.step()

        # Validation step for early stopping
        val_acc = calculate_accuracy(model, val_loader)
        if early_stopping.step(val_acc, model, epoch):
            print(f"Early stopping at epoch {epoch}")
            break
    return model

def calculate_accuracy(model, loader):
    model.eval()
    correct = sum((model(data).argmax(dim=1) == data.y).sum().item() for data in loader)
    total = sum(len(data.y) for data in loader)
    return 100 * correct / total

"""# **Graph data loader**"""

import os
import pandas as pd
import torch
from torch_geometric.data import Data

class GraphDataLoader:
    def __init__(self, nodes_folder_path: str, edges_folder_path: str):
        self.nodes_folder_path = nodes_folder_path
        self.edges_folder_path = edges_folder_path
        self.node_data = None
        self.edge_data = None
        self.graph_data = None

    def load_data(self):
        # Load all node CSVs and concatenate
        node_files = [os.path.join(self.nodes_folder_path, f) for f in os.listdir(self.nodes_folder_path) if f.endswith('.csv')]
        node_dfs = [pd.read_csv(f) for f in node_files]
        self.node_data = pd.concat(node_dfs, ignore_index=True)

        # Load all edge CSVs and concatenate
        edge_files = [os.path.join(self.edges_folder_path, f) for f in os.listdir(self.edges_folder_path) if f.endswith('.csv')]
        edge_dfs = [pd.read_csv(f) for f in edge_files]
        self.edge_data = pd.concat(edge_dfs, ignore_index=True)

        # Keep only numeric columns in node and edge data
        self.node_data = self.node_data.select_dtypes(include=[int, float])
        self.edge_data = self.edge_data.select_dtypes(include=[int, float])

    def create_graph(self):
        # Convert node and edge data to torch tensors
        node_features = torch.tensor(self.node_data.values, dtype=torch.float)
        edge_indices = torch.tensor(self.edge_data.values.T, dtype=torch.long)

        # Create the graph data object
        self.graph_data = Data(x=node_features, edge_index=edge_indices)

    def get_graph_data(self):
        if self.graph_data is None:
            raise ValueError("Graph data has not been created. Call 'load_data()' and 'create_graph()' first.")
        return self.graph_data

# Example usage
nodes_path = '/content/MetaModel_EXTRA/csvfiles/nodes' # Adjust this path based on your directory structure
edges_path = '/content/MetaModel_EXTRA/csvfiles/edges'

graph_loader = GraphDataLoader(nodes_path, edges_path)
graph_loader.load_data()
graph_loader.create_graph()

graph_data = graph_loader.get_graph_data()

# You can now use this graph_data for further processing
print(graph_data)

# Now print the edge indices before and after fixing
print(f"Edge indices (edge_index) before fixing: {graph_data.edge_index.size()}")
print(f"Edge indices content:\n{graph_data.edge_index}")



# Reshape to (2, -1)
edge_index = graph_data.edge_index.reshape(2, -1)
print(edge_index.shape)

# Check node feature matrix size and edge index size
print(f"Node feature matrix size (num_nodes): {graph_data.x.size()}")
print(f"Edge index size: {graph_data.edge_index.size()}")
print(f"Edge index content:\n{graph_data.edge_index}")

# Check the number of nodes and reshape the edge_index if needed
if graph_data.edge_index.size(0) != 2:
    print(f"Reshaping edge_index from {graph_data.edge_index.size()} to (2, -1)")
    graph_data.edge_index = graph_data.edge_index[:2, :]  # Ensure edge_index has two rows (source, destination)
# Recheck after reshaping (if applied)
print(f"Edge index size after fixing: {graph_data.edge_index.size()}")
print(f"Edge index content after fixing:\n{graph_data.edge_index}")

from torch_geometric.utils import add_remaining_self_loops

# Add self-loops after fixing the edge_index
num_nodes = graph_data.x.size(0)
edge_index, _ = add_remaining_self_loops(graph_data.edge_index, num_nodes=num_nodes)

# Update the edge_index in the data object and print again
graph_data.edge_index = edge_index
print(f"Updated Edge indices (with self-loops): {graph_data.edge_index.size()}")
print(f"Updated Edge indices content:\n{graph_data.edge_index}")

"""# Metric"""

from scipy import stats
from sklearn.metrics import accuracy_score

def run_c2st_test(model, test_loader):
    model.eval()
    preds, labels = [], []
    for data in test_loader:
        output = model(data)
        preds.extend(output.argmax(dim=1).cpu().numpy())
        labels.extend(data.y.cpu().numpy())

    test_accuracy = accuracy_score(labels, preds)
    p_value = calculate_p_value(test_accuracy, len(labels))
    print(f"Test Accuracy: {test_accuracy:.2f}, p-Value: {p_value:.4f}")
    return test_accuracy, p_value

def calculate_p_value(accuracy, sample_size):
    return 1 - stats.norm.cdf((accuracy - 0.5) / np.sqrt(1 / (4 * sample_size)))

# Define model parameters based on your dataset
in_channels = 10  # Set this to the number of features in your node data
hidden_channels = 32  # Set this as a hyperparameter
out_channels = 2  # Set this to the number of classes (e.g., 2 for binary classification)

# Initialize and train the model
model = GNNWithAttention(in_channels, hidden_channels, out_channels)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

import torch
from torch_geometric.data import Data, DataLoader

# Example function to prepare your dataset (assuming you have node_data, edge_data, and labels)
def create_graph_data(node_data, edge_data, labels):
    node_features = torch.tensor(node_data.values, dtype=torch.float)
    edge_indices = torch.tensor(edge_data.values.T, dtype=torch.long)
    y_labels = torch.tensor(labels.values, dtype=torch.long)

    return Data(x=node_features, edge_index=edge_indices, y=y_labels)

# Assuming node_data, edge_data, labels for each set
train_data = create_graph_data(train_node_data, train_edge_data, train_labels)
val_data = create_graph_data(val_node_data, val_edge_data, val_labels)
test_data = create_graph_data(test_node_data, test_edge_data, test_labels)

# Create DataLoader objects for each set
train_loader = DataLoader([train_data], batch_size=32, shuffle=True)
val_loader = DataLoader([val_data], batch_size=32)
test_loader = DataLoader([test_data], batch_size=32)

# Assuming train_loader and val_loader are defined
trained_model = train_c2st(model, train_loader, val_loader, optimizer, epochs=50)

import os
import yaml
import torch
import pandas as pd
from torch_geometric.data import Data, DataLoader
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
from tqdm import tqdm
from torch_geometric.utils import add_remaining_self_loops

# Function to load model configuration from YAML
def load_model_config(yaml_file_path):
    """Loads model configuration from a YAML file."""
    with open(yaml_file_path, 'r') as file:
        config = yaml.safe_load(file)
    return config

# Function to preprocess the data (handle NaN and non-numeric columns)
def preprocess_data(df):
    """Preprocesses data by filling NaN values and selecting numeric columns only."""
    df = df.fillna(0)  # Fill missing values with 0
    df = df.select_dtypes(include=[int, float])  # Keep only numeric columns
    df = df.astype(float)  # Ensure all values are floats
    return df

# Function to load node and edge data from CSV files in a specified folder
def load_graph_data(csv_folder_path):
    """Loads node and edge data from CSV files in a specified folder."""
    node_data, edge_data = None, None
    folders = os.listdir(csv_folder_path)

    for folder in folders:
        files = os.listdir(os.path.join(csv_folder_path, folder))
        for csv_file in files:
            file_path = os.path.join(csv_folder_path, folder, csv_file)
            if folder == 'nodes':
                node_data = pd.read_csv(file_path) if node_data is None else pd.concat([node_data, pd.read_csv(file_path)], ignore_index=True)
            elif folder == 'edges':
                edge_data = pd.read_csv(file_path) if edge_data is None else pd.concat([edge_data, pd.read_csv(file_path)], ignore_index=True)

    node_data = preprocess_data(node_data)
    edge_data = preprocess_data(edge_data)

    return node_data, edge_data

# Function to create a PyTorch Geometric Data object with node features and edge indices
def create_graph_data(node_data, edge_data, labels):
    """Creates a PyTorch Geometric Data object with node features, edge indices, and labels."""
    if node_data is not None and edge_data is not None and labels is not None:
        # Ensure edge index is properly shaped (source, destination) format
        edge_index = torch.tensor(edge_data.values.T, dtype=torch.long)
        if edge_index.size(0) != 2:
            print(f"Reshaping edge_index from {edge_index.size()} to (2, -1)")
            edge_index = edge_index[:2, :]  # Ensure edge index has two rows (source, target)
            # edge_index = edge_index.reshape(2, -1)
        graph_data = Data(
            x=torch.tensor(node_data.values, dtype=torch.float32),
            edge_index=edge_index,
            y=torch.tensor(labels.values, dtype=torch.long)
        )
        return graph_data
    else:
        raise ValueError("Node data, Edge data, or Labels are missing.")

# Function to calculate accuracy based on predictions and targets
def calculate_accuracy(predictions, targets):
    """Calculates accuracy of the model."""
    _, predicted_labels = torch.max(predictions, 1)
    correct = (predicted_labels == targets).sum().item()
    accuracy = correct / targets.size(0) * 100
    return accuracy

# Training function with tabular results
def train_model_with_accuracy(model, train_data, learning_rate, epochs, batch_size):
    """Trains the model and tracks accuracy and loss over epochs."""
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

    epoch_losses = []
    epoch_accuracies = []
    epoch_errors = []
    best_accuracy = 0.0
    best_epoch = 0

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0
        total_batches = len(train_loader)
        total_accuracy = 0

        with tqdm(total=total_batches, desc=f'Training Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:
            for batch_idx, data in enumerate(train_loader):
                optimizer.zero_grad()

                # Forward pass: get predictions
                predictions, _ = model(data.x, data.edge_index)

                # Ensure `data.y` contains valid labels
                if data.y is None:
                    raise ValueError("Missing target labels (data.y) for loss and accuracy calculation.")

                # Compute loss
                loss = F.cross_entropy(predictions, data.y)
                loss.backward()
                optimizer.step()

                # Update loss
                epoch_loss += loss.item()

                # Calculate batch accuracy and error
                accuracy = calculate_accuracy(predictions, data.y)
                total_accuracy += accuracy

                pbar.update(1)
                pbar.set_postfix(loss=loss.item(), accuracy=f'{accuracy:.2f}%')

        avg_loss = epoch_loss / total_batches
        avg_accuracy = total_accuracy / total_batches
        avg_error = 100 - avg_accuracy

        epoch_losses.append(avg_loss)
        epoch_accuracies.append(avg_accuracy)
        epoch_errors.append(avg_error)

        # Track best accuracy
        if avg_accuracy > best_accuracy:
            best_accuracy = avg_accuracy
            best_epoch = epoch + 1

        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%, Error: {avg_error:.2f}%')

    print(f'\nBest Accuracy: {best_accuracy:.2f}% achieved at Epoch {best_epoch}')

    # Create a DataFrame to display in tabular form
    results_df = pd.DataFrame({
        'Epoch': range(1, epochs + 1),
        'Loss': epoch_losses,
        'Accuracy (%)': epoch_accuracies,
        'Error (%)': epoch_errors
    })

    print("\nTraining Summary:")
    print(results_df.to_string(index=False))

    return epoch_losses, epoch_accuracies, best_accuracy, best_epoch

"""# **check if CUDA is available in your environment (whether you're using a GPU)** **and** **check the version of PyTorch installed in your environment**

"""

import torch

if torch.cuda.is_available():
    print("CUDA is available. You're using a GPU!")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
else:
    print("CUDA is not available. You're using a CPU.")

print(torch.__version__)

"""# **Start Training Model Graphs**
**best accuracy and epoch**
"""

if __name__ == "__main__":
    try:
        # Load the model configuration from the YAML file
        yaml_file_path = '/content/drive/MyDrive/gnnmodel/model (2).yaml'
        model_config = load_model_config(yaml_file_path)

        # Load and preprocess data from the specified CSV folder
        node_data, edge_data = load_graph_data('/content/MetaModel_EXTRA/csvfiles')

        # Use the 'graphNumber' column from the node data as labels
        labels = node_data['graphNumber']

        # Create the graph data object (PyTorch Geometric Data object)
        graph_data = create_graph_data(node_data, edge_data, labels)

        # Ensure edge index is in correct format and add self-loops
        num_nodes = graph_data.x.size(0)
        graph_data.edge_index, _ = add_remaining_self_loops(graph_data.edge_index, num_nodes=num_nodes)

        # Initialize the GCN model based on YAML configuration
        model = GCN(
            num_layers=model_config['hidden_layers'],
            layer_dims=model_config['layer_dimensions'],
            activation_func=model_config['activation_function'],
            dropout_rate=model_config['dropout']  # Ensure dropout rate is passed from the YAML config
        )

        # Train the model using the provided configuration and graph data
        epoch_losses, epoch_accuracies, best_accuracy, best_epoch = train_model_with_accuracy(
            model, [graph_data],  # Wrap the graph_data in a list for the DataLoader
            learning_rate=model_config['learning_rate'],
            epochs=model_config['epochs'],
            batch_size=model_config['batch_size']
        )

        # Output the best accuracy and corresponding epoch
        print(f"Best Accuracy: {best_accuracy:.2f}% at epoch {best_epoch}")

    except Exception as e:
        print(f"An error occurred during execution: {e}")

"""# **Plotting loss/acc**"""

import matplotlib.pyplot as plt

def plot_losses_together(losses, accuracies):
    """Plots training losses and accuracies with line and scatter in one figure"""
    fig, axs = plt.subplots(1, 2, figsize=(15, 4))  # 2x1 grid of subplots

    # Loss plot (line and scatter on the same axes)
    epochs = range(1, len(losses) + 1)
    axs[0].plot(epochs, losses, label='Training Loss (line)', color='blue', linestyle='-')
    axs[0].scatter(epochs, losses, color='red', label='Training Loss (scatter)')
    axs[0].set_title('Loss Over Epochs')
    axs[0].set_xlabel('Epoch')
    axs[0].set_ylabel('Loss')
    axs[0].grid(True)
    axs[0].legend()

    # Accuracy plot (line and scatter on the same axes)
    axs[1].plot(epochs, accuracies, label='Training Accuracy (line)', color='green', linestyle='-')
    axs[1].scatter(epochs, accuracies, color='orange', label='Training Accuracy (scatter)')
    axs[1].set_title('Accuracy Over Epochs')
    axs[1].set_xlabel('Epoch')
    axs[1].set_ylabel('Accuracy (%)')
    axs[1].grid(True)
    axs[1].legend()

    plt.tight_layout()
    plt.show()

# Example usage with sample data (you can replace these with your actual `epoch_losses` and `epoch_accuracies` lists)
plot_losses_together(epoch_losses, epoch_accuracies)

"""# **Loss curve using different charts**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set a more attractive style using seaborn
sns.set(style="whitegrid")

# Plotting function for training losses with different chart styles
def plot_losses_together(losses):
    """Plots training losses using different types of plots with enhanced visuals"""
    fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # 2x2 grid of subplots

    # Line plot with markers
    axs[0, 0].plot(range(1, len(losses) + 1), losses, label='Training Loss', color='royalblue', marker='o', markersize=6, linestyle='--')
    axs[0, 0].set_title('Line Plot', fontsize=14, fontweight='bold')
    axs[0, 0].set_xlabel('Epoch', fontsize=12)
    axs[0, 0].set_ylabel('Loss', fontsize=12)
    axs[0, 0].legend(loc='best')
    axs[0, 0].grid(True, linestyle='--', alpha=0.7)

    # Scatter plot with larger markers
    axs[0, 1].scatter(range(1, len(losses) + 1), losses, label='Training Loss', color='crimson', s=70, alpha=0.8)
    axs[0, 1].set_title('Scatter Plot', fontsize=14, fontweight='bold')
    axs[0, 1].set_xlabel('Epoch', fontsize=12)
    axs[0, 1].set_ylabel('Loss', fontsize=12)
    axs[0, 1].legend(loc='best')
    axs[0, 1].grid(True, linestyle='--', alpha=0.7)

    # Bar plot with alpha transparency
    axs[1, 0].bar(range(1, len(losses) + 1), losses, label='Training Loss', color='mediumseagreen', alpha=0.85)
    axs[1, 0].set_title('Bar Plot', fontsize=14, fontweight='bold')
    axs[1, 0].set_xlabel('Epoch', fontsize=12)
    axs[1, 0].set_ylabel('Loss', fontsize=12)
    axs[1, 0].legend(loc='best')
    axs[1, 0].grid(True, linestyle='--', alpha=0.7)

    # Histogram plot with KDE overlay
    sns.histplot(losses, bins=10, kde=True, color='mediumpurple', ax=axs[1, 1], alpha=0.7)
    axs[1, 1].set_title('Histogram with KDE', fontsize=14, fontweight='bold')
    axs[1, 1].set_xlabel('Loss Value', fontsize=12)
    axs[1, 1].set_ylabel('Frequency', fontsize=12)
    axs[1, 1].grid(True, linestyle='--', alpha=0.7)

    # Adjust layout for better spacing
    plt.tight_layout(pad=3.0)
    plt.show()

plot_losses_together(epoch_losses)

"""# **Accuracy curve using different graphs**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set an attractive style using seaborn
sns.set(style="whitegrid")

# Enhanced plotting function for accuracies with multiple plot types in one frame
def plot_accuracies_together(accuracies):
    """
    Plots training accuracies using different types of plots with enhanced visuals
    Args:
    accuracies: List of accuracy values to plot for each epoch
    """
    fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # 2x2 grid of subplots, larger size

    # Line plot with markers for accuracy
    axs[0, 0].plot(range(1, len(accuracies) + 1), accuracies, label='Training Accuracy', color='royalblue', marker='o', markersize=6, linestyle='--')
    axs[0, 0].set_title('Accuracy Over Epochs - Line Plot', fontsize=14, fontweight='bold')
    axs[0, 0].set_xlabel('Epoch', fontsize=12)
    axs[0, 0].set_ylabel('Accuracy (%)', fontsize=12)
    axs[0, 0].legend(loc='best')
    axs[0, 0].grid(True, linestyle='--', alpha=0.7)

    # Scatter plot with larger markers for accuracy
    axs[0, 1].scatter(range(1, len(accuracies) + 1), accuracies, label='Training Accuracy', color='crimson', s=70, alpha=0.8)
    axs[0, 1].set_title('Accuracy Over Epochs - Scatter Plot', fontsize=14, fontweight='bold')
    axs[0, 1].set_xlabel('Epoch', fontsize=12)
    axs[0, 1].set_ylabel('Accuracy (%)', fontsize=12)
    axs[0, 1].legend(loc='best')
    axs[0, 1].grid(True, linestyle='--', alpha=0.7)

    # Bar plot for accuracy
    axs[1, 0].bar(range(1, len(accuracies) + 1), accuracies, label='Training Accuracy', color='mediumseagreen', alpha=0.85)
    axs[1, 0].set_title('Accuracy Over Epochs - Bar Plot', fontsize=14, fontweight='bold')
    axs[1, 0].set_xlabel('Epoch', fontsize=12)
    axs[1, 0].set_ylabel('Accuracy (%)', fontsize=12)
    axs[1, 0].legend(loc='best')
    axs[1, 0].grid(True, linestyle='--', alpha=0.7)

    # Histogram plot with KDE for accuracy distribution
    sns.histplot(accuracies, bins=10, kde=True, color='mediumpurple', ax=axs[1, 1], alpha=0.7)
    axs[1, 1].set_title('Accuracy Distribution - Histogram with KDE', fontsize=14, fontweight='bold')
    axs[1, 1].set_xlabel('Accuracy (%)', fontsize=12)
    axs[1, 1].set_ylabel('Frequency', fontsize=12)
    axs[1, 1].grid(True, linestyle='--', alpha=0.7)

    # Adjust layout for better spacing
    plt.tight_layout(pad=3.0)
    plt.show()

# Example usage: Replace accuracies with your actual accuracy data from training
# accuracies = [80, 85, 88, 90, 92, 93, 94, 95, 96, 97]  # Replace this with your accuracy data
plot_accuracies_together(epoch_accuracies)